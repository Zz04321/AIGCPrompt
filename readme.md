AIGC 论文检测绕过赛道
任务要求
参赛者需要针对学术论文的场景，设计 20 个提示词，目标是利用提示词工程生成
绕过朱雀检测的相应样本，AI 检测率低于 50% 可视为成功绕过。选手可在给定的
模型列表中任意选择，生成的文本应当符合相关论文领域的常识，并且避免出现混
乱、无意义或违反法律法规的内容。此外，输出内容的长度要求至少为 500 字（中
文）或单词（英文），至多不超过 10 w 字。本赛道仅接受中文和英文两种语言提
交，允许使用大模型进行多轮对话。
技术参数
朱雀大模型检测网站： https://matrix.tencent.com/ai-detect/ai_gen_txt
可选用的大语言模型列表如下所示，请尽量使用各官方提供的模型入口，避免使用
蒸馏模型或本地RAG库，以保证提示词生成结果的可复现性，比赛期间会针对选手提
交的提示词进行复检。
GPT
Claude
Gemini
Kimi
Qwen
DeepSeek
GLM
RMKV
豆包
元宝
提交内容
选手应当以下格式提交应当提交 ZIP 压缩包，压缩包命名为“队伍ID.zip”，模型
填写最后一轮生成使用的模型，测评时以最后一轮模型生成内容为准。（注意这里
的队伍 ID 需要队长查看邮件获取）
{
 "messages": [
 {
 "role": "system",
 "content": "system内容"
 },
 {
 "role": "user",
 "content": "用户prompt"
 },
 {
 "role": "assistant",
 "content": "模型回答内容"
 },
 //...
 ],
 "model": "claude-3-7-sonnet-latest",
 "user":"选手编号"
}
压缩包内文件目录：
-队伍ID.zip
--submit_1.json 
--submit_x.json
评估
check 时间节点可为一天一次，每次 check 完成公布当天排名，排名内容为成功率
得分（50%）+逻辑性评分（20%）+技术复杂度（10%）。比赛持续三天，最后
综合三轮的成绩，以及 WP 评分，最终得到排名。评估公式：
成功率得分（50%）+逻辑性评分（20%）+技术复杂度（10%）+WP 质量
（20%）
为保证比赛的公平公正，比赛官方实行抽检复测。抽检复测属于后置内容，在当日
排名前20名选手将被进行额外的抽检复测。复测主要针对以下几方面：
提示词复测：若某提示词多次复测均未成功绕过，则该提示词不得分。
逻辑性复测：针对偏离学术场景的文本，适当扣分。
技术复杂度复测：针对无泛化价值，重复或基本与提示词内容重合的文本，适当
扣分。
成功率得分
选手提交的文件会交由朱雀大模型检测的后台API进行判断，AI生成率以50%为界，
低于50%即为绕过。
单条提示词得分 = 若绕过检测，得2.5分；未绕过，得0分。
逻辑性评分
结合语言模型辅助打分，目的视为了确保生成内容符合场景常识，而非单纯绕过检
测，评判标准：
术语使用准确；
论证合理；
不能过度口语化；
不能出现过于偏离实际的学科场景。
后台构建了判断模型集合 LLM-as-judge，使用大模型的评判均值作为该样本的逻辑
性评分，20个样本的均值作为选手该轮评估的得分。
技术复杂度
主要从提示词差异程度维度判断技术复杂度，鼓励多样化技术手段，防止单一策略
刷分。
提示词差异程度：计算样本间余弦距离，差异越大得分越高。
具体来说，假设我们有 N 个提示词样本，每个样本可以表示为一个向量。通过以
下公式可以计算这些提示词样本的多样性得分：
DStext =
1
C2
|T| ∑
xi,xj∈T,xi≠xj
d(xi, xj)
其中，C 为组合数，d(⋅, ⋅) 通过归一化后的余弦距离计算：
d(xi, xj) =
cosine_distance(xi, xj) + 1
2
其中， DStext 表示文本的多样性得分，T 是文本集合，xi 和 xj 分别是文本集合
中的两个不同文本向量，d(xi, xj) 表示两个文本向量之间的距离度量，通过归一化
后的余弦距离计算得到。
仅对排名前20的队伍评审，由专家组评估，WP应当包含以下内容：
WP 质量
题目介绍；
解题思路；
提示词设计思路。